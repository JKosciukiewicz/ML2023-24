{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUM 2023-24 Graf Obliczeniowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graf obliczeń\n",
    "\n",
    "* Jest to graf skierowany acykliczny DAG\n",
    "  * Każdy wierzchołek oznacza wykonanie pewnej funkcji na wartościach wierzchołków połączonych krawędziami wchodzącymi\n",
    "  * Wszystkie wierzchołki bez krawędzi wchodzących traktujemy jako input grafu.\n",
    "* Do tej pory rozważaliśmy funkcje operujące na wektorach i zwracające wektory\n",
    "  * W praktyce będziemy jednak używali operacji, które przyjmują na wejściu wielowymiarowe tablice float'ów i zwracają jedną tablicę. Wymiary poszczególnych tablic mogą się różnić.\n",
    "* Celem grafu obliczeń jest zareprezentowanie (skomplikowanej) funkcji wielokrotnie złożonej w sposób, który umożliwi jej automatyczne różniczkowanie\n",
    "  * Graf będzie składał się z __modelu__ oraz __funkcji kosztu__\n",
    "  * wejście grafu wykorzystamy do wprowadzenia __danych treningowych__ oraz __parametrów modelu__. Naszym celem będzie minimalizowanie funkcji kosztu poprzez gradientową optymalizację parametrów modelu.\n",
    "<img style=\"float: right;\" src=\"ml_figures/Graf_obliczen_graf1.png\" width=400>\n",
    "\n",
    "* Umiemy różniczkować funkcje złożone (mnożenie jakobianów), ale jak policzyć jakobian, skoro funkcja operuje na tablicach, a nie wektorach?\n",
    "  * Otóż musielibyśmy najpierw rozwinąć wszystkie inputy i skleić je w jeden wektor, który funkcja $f$ przekształca w rozwinięty output\n",
    "  * Z reguły to podejście jest niepraktyczne, więc będziemy starali się działać sprytniej.\n",
    "<img style=\"float: right;\" src=\"ml_figures/Graf_obliczen_graf2.png\" width=650>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cel algorytmu\n",
    "\n",
    "* Mamy pewien input (dane, parametry modelu), który chcemy podać do grafu. Na jego podstawie obliczamy wartość funkcji kosztu, reprezentowaną przez wierzchołek $L$. Dla każdego wierzchołka typu input chcemy obliczyć gradient funkcji kosztu po tym wierzchołku, w punkcie wyznaczonym przez podane na input wartości liczbowe.\n",
    "* Gradienty będziemy oznaczali literą $\\delta$. W efekcie chcemy więc, aby w każdym wierzchołku typu input obliczona została tablica liczb $\\delta$ - jej wymiary są zgodne z podaną na input tablicą wartości liczbowych.\n",
    "* Oczywiście możemy policzyć gradienty również po innych wierzchołkach, które leżą na drodze pomiędzy inputami a wierzchołkiem $L$ - tak właśnie zrobimy, ponieważ to umożliwi nam obliczenie gradientów inputów.\n",
    "* Musimy wiedzieć, w jakim punkcie liczymy gradienty. W wypadku wierzchołków typu input są to po prostu odpowiednie wartości podawane na input. Natomiast w wypadku pozostałych wierzchołków musimy na podstawie inputu obliczyć i zapamiętać ich wartości liczbowe.\n",
    "* Podsumowując:\n",
    "  1. chcemy obliczyć i zapamiętać wartości liczbowe wszystkich wierzchołków, począwszy od inputu, skończywszy na funkcji kosztu\n",
    "  2. chcemy w tych wierzchołkach obliczyć wartości gradientu funkcji kosztu, oznaczamy je symbolem $\\delta$\n",
    "  3. chcemy zwrócić gradienty policzone w wierzchołkach typu input, a w szczególności w wierzchołkach opisujących parametry modelu\n",
    "* będziemy uczyć model gradientowo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład\n",
    "\n",
    "* Regresja liniowa. Naszym zadaniem jest dopasowanie prostej do $N$-elementowego zbioru punktów treningowych $(x_n, y_n)$.\n",
    "<img style=\"float: left;\" src=\"ml_figures/Graf_obliczen_linreg2.png\" width=450>\n",
    "\n",
    "  * Równanie prostej to $\\widehat{y} = ax + b$.\n",
    "  * Niech $\\mathbf{x}$ oznacza wektor złożony ze wszystkich $x_n$, natomiast $\\mathbf{y}$ wektor wszystkich $y_n$.\n",
    "  * Funkcję kosztu definiuje się jako $L(a,b) = \\dfrac{1}{N}\\sum_{n=1}^N (ax_n+b-y_n)^2$.\n",
    "\n",
    "<img style=\"float: right;\" src=\"ml_figures/Graf_obliczen_linreg.png\" width=400>\n",
    "\n",
    "  * Dwuelementowy wektor $\\theta=(a,b)$ to parametry modelu.\n",
    "  * Dla tak postawionego problemu graf obliczeń wygląda następująco:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "* Funkcję kosztu będziemy z reguły oznaczali literą $L$ (_loss function_).\n",
    "* __Node__ weźmy dowolny wierzchołek, który opisuje funkcję $f$. Niech jej input to ciąg tablic $X_1, \\ldots, X_K$, a output to tablica $Y$. Wierzchołek w grafie musi implementować dwie metody.\n",
    "* __Forward pass__ to po prostu implementacja funkcji $f$. Metoda przyjmuje $X_1, \\ldots, X_K$ i zwraca $Y$\n",
    "* __Backward pass__ metoda przyjmuje $X_1, \\ldots, X_K$ oraz $\\delta_Y$ i zwraca $\\delta_{X_1}, \\ldots, \\delta_{X_K}$. Tablica $\\delta_Y$ ma takie same wymiary jak $Y$ i przechowuje gradient $L$ po $Y$. Metoda ma obliczyć i zwrócić gradient $L$ po inputach. Jak to zrobić?\n",
    "  * Niech $x$ będzie pewnym elementem tablicy $X_k, k\\in\\{1,\\ldots,K\\}$. Trzeba sprawdzić, jaki ma on wpływ na wszystkie elementy outputu (odpowiednia pochodna cząstkowa funkcji $f$), a następnie przemnożyć je przez odpowiadające elementy $\\delta_Y$ i zsumować - w ten sposób policzymy, jak łącznie wpływa on na $L$ za pośrednictwem $f$. Uzyskaną liczbę wpisujemy w odpowiednie miejsce tablicy $\\delta_{X_k}$.\n",
    "  * Procedurę powtarzamy dla wszystkich elementów $X_k$, a następnie dla wszystkich $k$.\n",
    "* Wartości $X_1, \\ldots, X_K$ są potrzebne, ponieważ pochodna cząstkowa jest funkcją i musi być liczona w punkcie.\n",
    "  * Dla różnych wartości $X_1, \\ldots, X_K$ wpływ tego samego elementu inputu na pewien element outputu może się znacznie różnić, bo funkcja $f$ w różnych miejscach swojej dziedziny ma inne liniowe przybliżenie.\n",
    "\n",
    "#### Przykład: numpy.multiply\n",
    "* Niech $f$ oznacza funkcję, która przyjmuje dwie tablice (tego samego rozmiaru) $X_1, X_2$ i zwraca ich iloczyn element-wise $Y$.\n",
    "  * Niech $x_1$ oznacza pewien element $X_1$, natomiast $x_2, y, \\delta_y$ oznaczają elementy odpowiednio $X_2, Y, \\delta_Y$, które mają te same współrzędne, co $x_1$ w $X_1$.\n",
    "  * Z definicji $f$ zachodzi $y = x_1 x_2$. Ponadto $x_1, x_2$ nie mają wpływu na pozostałe elementy $Y$. W takim razie\n",
    "$$ \\delta_{x_1} = \\dfrac{\\partial L}{\\partial y}(y)\\cdot \\dfrac{\\partial y}{\\partial x_1}(x_1, x_2) = \\delta_y \\cdot x_2 $$\n",
    "$$ \\delta_{x_2} = \\dfrac{\\partial L}{\\partial y}(y)\\cdot \\dfrac{\\partial y}{\\partial x_2}(x_1, x_2) = \\delta_y \\cdot x_1 $$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_pass(X1, X2):\n",
    "    assert X1.shape == X2.shape\n",
    "    Y = np.multiply(X1, X2)\n",
    "    return Y\n",
    "\n",
    "def backward_pass(dY, X1, X2):\n",
    "    assert X1.shape == X2.shape == dY.shape\n",
    "    dX1 = np.multiply(dY, X2)\n",
    "    dX2 = np.multiply(dY, X1)\n",
    "    return dX1, dX2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jeśli np. tablice $X_1, X_2, Y$ mają rozmiar $(4,3)$, to jakobian $f$ ma rozmiar $(12,24)$, przy czym jedynie 24 z 288 elementów jakobianu jest niezerowych (bo na każdy $y$ mają niezerowy wpływ tylko dwie rzeczy).\n",
    "  * Widzimy, że znacznie łatwiej jest zaimplementować backward pass bez mnożenia explicite $\\delta_Y$ przez jakobian.\n",
    "* __Graph__\n",
    "  1. Wprowadź do grafu obliczeń zbiór treningowy oraz bieżące parametry modelu.\n",
    "  2. Forward pass: przelicz po kolei wszystkie funkcje (wierzchołki) w grafie, aż do momentu uzyskania wartości funkcji kosztu. Zapamiętaj w każdym pośrednim wierzchołku obliczone wartości - wraz z inputem grafu będą one potrzebne do policzenia pochodnej.\n",
    "  3. Ustaw $\\delta_L = 1$. (dlaczego?)\n",
    "  4. Dla każdego wierzchołka $W$:\n",
    "    1. Niech $V_1, \\ldots, V_K$ będą wszystkimi wierzchołkami, których inputem jest $W$.\n",
    "    2. Dla każdego wierzchołka $V_k$ wykonaj krok 4. rekurencyjnie - oblicz i zapisz $\\delta_{V_k}$.\n",
    "    3. Dla każdego wierzchołka $V_k$ wywołaj jego metodę backward pass (użyj zapisanych wartości $\\delta_{V_k}$ oraz warości inputów zapamiętanych w kroku 2.). Zapamiętaj wszystkie $\\delta_W^k$.\n",
    "    4. Zsumuj $k$ zapamiętanych tablic $\\delta_W^k$, zapisz sumę jako $\\delta_W$. (dlaczego suma?)\n",
    "\n",
    "![graf3](../ml_figures/Graf_obliczen_graf3.png)\n",
    "\n",
    "* __Training loop__\n",
    "  * Uczenie przebiega w pętli.\n",
    "  * Wykonujemy pełny forward pass, a następnie backward pass.\n",
    "  * Delty parametrów modelu (gradient funkcji kosztu) przekazujemy do optimizera.\n",
    "  * Optimizer oblicza nowe wartości parametrów, które użyjemy w następnym kroku.\n",
    "  * Uczenie kończymy np. po określonej liczbie kroków lub po osiągnięciu satysfakcjonującej wartości funkcji kosztu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autograd\n",
    "* w praktyce wszystkie gradienty są obliczane automatycznie\n",
    "* wystarcza do tego zestaw podstawowych informacji oraz implementacja reguły łańcuchowej\n",
    "* pakiet `torch.autograd` (czy innej implementacji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
