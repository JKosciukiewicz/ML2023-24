{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUM 2023-24 Optymalizatory adaptacyjne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"ml_figures/conv_nn.png\" width=450>\n",
    "\n",
    "* wartości bezwzględne gradientów mogą się bardzo różnić między warstwami dużych sieci neuronowych\n",
    "  * globalne $\\gamma_t$ może nie działać poprawnie\n",
    "  * śledzić i uaktualniać współczynnik uczenia dla każdego parametru\n",
    "  * algorytmy adaptują się do _wariancji_ wag albo lokalnej krzywizny problemu (powierzchni błędu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uwaga\n",
    "\n",
    "1. W oryginalnych pracach dotyczących niektórych z poniższych metod pojawiają się matematyczne \"akrobacje\", które polegają na przykład na definiowaniu \"pierwiastka ze zdiagonalizowanej macierzy produktu zewnętrznego historii gradientów\". Proszę absolutnie nie myśleć w ten sposób o tych optimizerach!\n",
    "\n",
    "2. W tym notebooku wszystkie operacje na wektorach są zdefiniowane _element-wise_, zgodnie z regułami pakietu numpy. Na przykład:\n",
    "    * kwadrat wektora to wektor kwadratów elementów (numpy.square)\n",
    "    * pierwiastek z wektora to wektor pierwiastków z elementów (numpy.sqrt)\n",
    "    * suma, różnica, iloraz, iloczyn - analogicznie\n",
    "    * suma wektora i liczby oznacza dodanie tej liczby do każdej współrzędnej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "<img style=\"float: right;\" src=\"ml_figures/opt2.gif\" width=450>\n",
    "\n",
    "* $\\gamma_t$ - współczynnik uczenia (learning rate), typowo od 0.001 do 0.01\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero (zwykle $10^{-8}$)\n",
    "* $h_t$ - wektor sumujący kwadraty gradientów do czasu $t$, wymiar taki sam jak $w$; $h_t(0)=0$\n",
    "$$\\begin{align}\n",
    "h_{t+1} &= h_t + \\nabla L(w_t)^2\\\\\n",
    "w_{t+1}&=w_t - \\gamma_t \\dfrac{\\nabla L(w_t)}{\\sqrt{h_{t+1} + \\epsilon}}\n",
    "\\end{align}$$\n",
    "1. dzielenie normalizuje gradient\n",
    "2. normalizacja oddzielnie dla:\n",
    "    * każdej współrzędnej gradientu\n",
    "      * czyli dla każdej współrzędnej $w$ - osobno dla każdego parametru modelu\n",
    "3. $h_t$ jest sumą kwadratów, więc trzeba w mianowniku wziąć pierwiastek - bez pierwiastka działa słabo\n",
    "4. __akumulacja__ kwadratów gradientów\n",
    "    * $h$ to suma, a nie średnia\n",
    "    * gdyby gradienty były stałe, mianownik rósłby proporcjonalnie do $\\sqrt{t}$\n",
    "    * w praktyce maleje $\\Delta w$ i model może __przestać się uczyć__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp (Hinton)\n",
    "<img style=\"float: right;\" src=\"ml_figures/opt2.gif\" width=450>\n",
    "\n",
    "* normalizacja przez pierwiastek wartości oczekiwanej gradientu\n",
    "* $\\gamma$ - learning rate (typowe wartości: od 0.001 do 0.01)\n",
    "* $\\alpha$ - współczynnik średniej kroczącej (zazwyczaj: 0.9)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero (zazwyczaj: $10^{-8})$\n",
    "* $v_t$ - wektor średniej kroczącej kwadratów gradientów do czasu $t$, wymiar taki sam jak $w$\n",
    "  * $v_t$ jest estymacją drugiego momentu\n",
    "  * większość algorytmów używa średnich kroczących dla estymacji szumu, który zmienia się w czasie\n",
    "$$\\begin{align}\n",
    "v_{t+1} &= \\overbrace{\\alpha v_{t} + \\overbrace{(1-\\alpha)\\nabla L(w_t)^2}^{\\textrm{kwadrat po elementach (element-wise)}}}^{\\textrm{wykładnicza średnia krocząca}}\\\\\n",
    "w_{t+1}&=w_t - \\gamma\\frac{\\nabla L(w_t)}{\\sqrt{v_{t+1}} + \\epsilon}\n",
    "\\end{align}$$\n",
    "  * $\\epsilon$ zabezpiecza przed dzieleniem przez zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta\n",
    "<img style=\"float: right;\" src=\"ml_figures/opt2.gif\" width=450>\n",
    "\n",
    "* $\\alpha$ - współczynnik średniej kroczącej (zazwyczaj: 0.95)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero, umożliwia rozpoczęcie uczenia (zazwyczaj: od $10^{-6}$ do $10^{-2}$)\n",
    "\n",
    "* $v_t$ - wektor średniej kroczącej kwadratów gradientów do czasu $t$ element po elemencie\n",
    "* $d_t$ - wektor średniej kroczącej $\\Delta{}w$ do czasu $t$ element po elemencie\n",
    "$$\\begin{align}\n",
    "v_{t+1} &= \\alpha{}v_t + (1-\\alpha)(\\nabla L(w_t)^2\\\\\n",
    "w_{t+1} &=w_t - \\sqrt{d_t + \\epsilon}\\; \\dfrac{\\nabla L(w_t)}{\\sqrt{v_{t+1} + \\epsilon}}\\\\\n",
    "d_{t+1} &= \\alpha d_t + (1-\\alpha)(w_{t+1} - w_{t})^2\n",
    "\\end{align}$$\n",
    "1. rozszerzenie RMSProp (ale zaproponowane niezależnie jako poprawka Adagrad)\n",
    "2. zastąpienie learning rate przez __wektor__\n",
    "    * learning rate __proporcjonalny do__ średniego $\\Delta{}w$\n",
    "    * upodobnienie szybkości poprawek do poprawek\n",
    "    * __eliminacja__ stałej uczenia z parametrów\n",
    "3. ważna rola parametru $\\epsilon$\n",
    "    * nie tylko zapobiega dzieleniu przez zero\n",
    "    * umożliwia rozpoczęcie uczenia - $d_1$ jest większe od zera\n",
    "    * $\\sqrt{\\epsilon}$ wyznacza __dolne ograniczenie__ $\\sqrt{d_t +\\epsilon}$, a stąd uczenie nie zatrzymuje się"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Adam: a method for stochastic optimization, D.P. Kingma, J. Ba, 2015](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "* rodzaj RMSprop z elementem momentum\n",
    "* $\\gamma$ - learning rate (zazwyczaj: 0.001)\n",
    "* $\\beta_1$ - współczynnik średniej kroczącej pierwszego momentu (zazwyczaj: 0.9)\n",
    "* $\\beta_2$ - współczynnik średniej kroczącej pierwszego momentu (zazwyczaj: 0.999)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero (zazwyczaj: $10^{-8}$)\n",
    "* $m_t$ - wektor średniej kroczącej pierwszego momentu gradientu do czasu $t$\n",
    "* $v_t$ - wektor średniej kroczącej drugiego momentu gradientu do czasu $t$\n",
    "$$\\begin{align}\n",
    "m_{t+1} &= \\beta_1 m_t + (1-\\beta_1)(\\nabla L(w_t)\\\\\n",
    "v_{t+1} &= \\beta_2 v_t + (1-\\beta_2)(\\nabla L(w_t))^2\\\\\n",
    "\\widehat m &= \\dfrac{m_{t+1}}{1-\\beta_1^{(t+1)}}\\\\\n",
    "\\widehat v &= \\dfrac{v_{t+1}}{1-\\beta_2^{(t+1)}}\\\\\n",
    "w_{t+1} &= w_t - \\gamma\\dfrac{\\widehat m}{\\sqrt{\\widehat v} + \\epsilon}\n",
    "\\end{align}$$\n",
    "__Uwaga__ $\\beta^{t+1}$ to \"$\\beta$ do potęgi $t+1$\", a nie $\\beta$ w czasie $t+1$\n",
    "albo skrótowo w stabilnej wersji, która jest szybko osiągana\n",
    "$$\\begin{align}\n",
    "m_{t+1} &= \\beta_1 m_t + (1-\\beta_1)(\\nabla L(w_t)\\\\\n",
    "v_{t+1} &= \\beta_2 v_t + (1-\\beta_2)(\\nabla L(w_t))^2\\\\\n",
    "w_{t+1} &= w_t - \\gamma\\dfrac{m_t}{\\sqrt{v_{t+1}} + \\epsilon}\n",
    "\\end{align}$$\n",
    "  * do aktualizacji wag używany jest wektor estymujący $m_t$ przez średnią kroczącą\n",
    "    * średnia krocząca __zbiasowana__ w kierunku zera\n",
    "      * \"uśrednienia\" wprowadzają poprawkę: im później, tym poprawka mniejsza\n",
    "  * __gradient__ adaptowany\n",
    "    * krocząca średnia gradientów (pierwszy moment) - tłumione oscylacje\n",
    "    * krocząca średnia kwadratów gradientów (drugi moment) - normalizacja gradientów\n",
    "  * eksponencjalna średnia krocząca estymująca momentum jest równoważna standardowej postaci przy przeskalowaniu\n",
    "\n",
    "* Adam jest zwykle znacznie lepszy od SGD dla problemów, które są źle uwarunkowane, a zwykle są 😞\n",
    "  * często jednak Adam nie zbiega dla prostych problemów!\n",
    "* Adam ma przewagę nad RMSprop ze względu na uwzględnienie momentum\n",
    "* Adam nie jest dobrze zrozumiały teoretycznie\n",
    "* dla wielu problemów związanych z obrazami, na przykład ImageNet, daje gorsze wyniki generalizacji\n",
    "* wymaga więcej pamięci niż SGD\n",
    "* ma dwa parametry momentum, skąd może być potrzebne trochę tunowania\n",
    "  * zwykle warto wypróbować SGD z momentu oraz Adama z szeregiem różnych parametrów by wybrać\n",
    "\n",
    "[Wizualizacja uczenia dla wielu algorytmów [deeplearning.ai]](https://www.deeplearning.ai/ai-notes/optimization/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I wiele innych\n",
    "\n",
    "* AdaMax, Nadam, AMSGrad, ...\n",
    "* algorytmy uczenia mogą być specyficzne dla konkretnych architektur i zadań\n",
    "  * na przykład uczenie sprzętowych architektur wprost\n",
    "    * bardzo rzadko — nie są one dostosowane do uczenia\n",
    "    * implementacja uczenia i inferencji bardzo się różnią od siebie\n",
    "    * znacznie prościej jest uczyć symulowaną architekturą a znalezione wagi skopiować\n",
    "* uczenie NN nie musi być gradientowe\n",
    "  * gradientowe wymaga by funkcja kosztu była różniczkowalna\n",
    "    * z tego powodu nie możemy uczyć wprost by celem była jak najlepsza klasyfikacja\n",
    "* alternatywą jest uczenie _perturbacyjne_\n",
    "  * dla aktualnego przykładu wagi są _perturbowane_ o małe $-\\epsilon$ i $+\\epsilon$\n",
    "  * wybierana jest poprawka, która daje lepszy wynik\n",
    "* wiele zaawansowanych metod wykorzystywanych w płytszych sieciach neuronowych nie jest wykorzystywanych dla sieci głębokich\n",
    "  * metody przybliżania drugiej pochodnej nie dają wiele w porównaniu do metod będących uogólnieniami metody momentum\n",
    "  * wykorzystanie macierzy Hesjanu jest bardzo trudne ze względu na dużą liczbę parametrów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## niektóre cechy pozwalające na pierwszą decyzję o wyborze algorytmu [na podstawie deeplearning.ai]\n",
    "[Wizualizacja uczenia dla wielu algorytmów [deeplearning.ai]](https://www.deeplearning.ai/ai-notes/optimization/index.html)\n",
    "* **SGD**\n",
    "  * proste urównoleglenie, jednak powolne gdy pamięć GPU jest niewystarczająca\n",
    "  * SGD szybciej zbiega na dużych zbiorach danych ze względu na częstszą aktualizację\n",
    "  * lepsza aproksymacja gradientu ze względu na wykorzystanie redundancji danych\n",
    "  * najmniej użytej pamięci dla ustalonej wielkości batcha\n",
    "* **momentum**\n",
    "  * przyspiesza uczenie, wykorzystując minimalną modyfikację algorytmu\n",
    "  * więcej pamięci na batch niż SGD, ale sporo mniej niż RMSprop czy Adam\n",
    "* **RMSprop**\n",
    "  * adaptacyjne podejście zapobiega zbyt szybkiemu zanikaniu czy wybuchowi hiperparametru uczenia\n",
    "  * utrzymuje prędkości uczenia odpowiednie dla każdego parametru modelu (wagi sieci)\n",
    "  * więcej pamięci niż momentum, ale mniej niż Adam\n",
    "* **Adam**\n",
    "  * hiperparametry algorytmu mogą być zwykle ustawione na wartości domyślne i nie potrzebują dodatkowego dopasowania\n",
    "    * a jest ich wiele — stała uczenia, eksponencjalna stała zanikania dla momentum, etc.\n",
    "  * realizuje formę statystycznego podejścia **wyżarzania** (ang. annealing) z adaptacyjnymi krokami uczenia\n",
    "  * zużywa najwięcej pamięci na batch\n",
    "  * jest zwykle domyślnym optymalizatorem\n",
    "    * może być ważne przy porównywaniu rozwiązań"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
