{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"ml_figures/MLE-logo.png\" width=200>\n",
    "\n",
    "# MUM 2023-24 Uczenie Maximum Likelihood Estimation MLE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametry modelu\n",
    "<img style=\"float: left;\" src=\"ml_figures/maximum_likelihood_estimation.gif\" width=496>\n",
    "\n",
    "* __model__ ma __parametry__ - oznaczamy je $\\theta$\n",
    "  * w modelu sieci neuronowych będą to **wagi**\n",
    "  * nie mylić z __hiperparametrami__ - parametry są __uczone__, hiperparametry należy __wybrać ręcznie__\n",
    "  * hiperparametry mogą być optymalizowane, jednak **nie przez minimalizację funkcji kosztu**\n",
    "* __nauczony model__ ma ustalone wartości $\\widehat\\theta$\n",
    "* dla różnych ustalonych wartości $\\widehat\\theta$ nauczony model opisuje __różne rozkłady prawdopodobieństwa__\n",
    "* uczenie metodą Maximum Likelihood oznacza znalezienie takich wartości $\\widehat\\theta$, które **maksymalizują likelihood zbioru treningowego** dla uczonego modelu\n",
    "  * zbiór treningowy to $D_{Tr}$\n",
    "  * nazwa \"Maximum Likelihood Estimator\" (MLE) opisuje cały proces uczenia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model generatywny\n",
    "* model generatywny jest **statystycznym** modelem obliczającym rozkład łącznego prawdopodobieństwa $p(x,y)$ dla zmiennej **obserwowanej** x i zmiennej *docelowej** (target) y\n",
    "  * wartości generowane przez model zależą od parametrów, więc zamiast $p(\\mathbf{x})$ piszemy teraz\n",
    "$$p(\\mathbf{x}\\mid\\theta)$$\n",
    "    * zbiór treningowy to\n",
    "$$D_{Tr} = \\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\}$$\n",
    "  * **likelihood** zbioru treningowego dla ustalonych parametrów $\\widehat\\theta$ to\n",
    "$$p(\\mathbf{x}=\\mathbf{x}_1 \\mid\\theta=\\widehat\\theta)\\cdot\\ldots\\cdot p(\\mathbf{x}=\\mathbf{x}_N\\mid\\theta=\\widehat\\theta) = \\prod_{n=1}^N p(\\mathbf{x}=\\mathbf{x}_n \\mid\\theta=\\widehat\\theta)$$\n",
    "  * **uwaga** - jeśli model generatywny nie daje nam wprost wzoru na likelihood przykładu $p(\\mathbf{x}=\\mathbf{x}_n \\mid\\theta=\\widehat\\theta)$, to musimy umieć zdefiniować jakieś sprytne przybliżenie\n",
    "  * często jest to pisane $\\mathcal{L}(\\theta\\mid x)=p_\\theta(x)$\n",
    "    * **likelihood** $\\mathcal{L}$ modelu $p_\\theta(x)$ z parametryzacją przy danych $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model dyskryminatywny\n",
    "* model dyskryminatywny podaje rozkład prawdopodobieństwa **warunkowego** $p(x\\mid y)$ dla zmiennej celu y pod warunkiem zmiennej x\n",
    "  * zamiast $p(y\\mid\\mathbf{x})$ piszemy teraz\n",
    "$$p(y\\mid\\mathbf{x},\\theta)$$\n",
    "  * zbiór treningowy to\n",
    "$$D_{Tr} = \\{(\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_N, y_N)\\}$$\n",
    "  * likelihood zbioru treningowego dla ustalonych parametrów $\\widehat\\theta$ to\n",
    "$$p(y=y_1\\mid\\mathbf{x}=\\mathbf{x}_1,\\theta=\\widehat\\theta)\\cdot\\ldots\\cdot p(y=y_N\\mid\\mathbf{x}=\\mathbf{x}_N,\\theta=\\widehat\\theta) = \\prod_{n=1}^N p(y=y_n\\mid\\mathbf{x}=\\mathbf{x}_n,\\theta=\\widehat\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Log Likelihood\n",
    "<img style=\"float: left;\" src=\"ml_figures/maximum_likelihood_estimation.gif\" width=400>\n",
    "\n",
    "* powyżej pojawiają się iloczyny $N$ prawdopodobieństw\n",
    "* jeśli $N$ jest duże (zazwyczaj jest _bardzo_ duże), obliczenia mogą być niestabilne numerycznie\n",
    "* logarytm (naturalny) pozwala zamienić iloczyn na sumę\n",
    "    * model generatywny\n",
    "        $$\\ln\\left(\\prod_{n=1}^N p(\\mathbf{x}=\\mathbf{x}_n \\mid\\theta=\\widehat\\theta)\\right) = \\sum_{n=1}^N\\ln[p(\\mathbf{x}=\\mathbf{x}_n \\mid\\theta=\\widehat\\theta)]$$\n",
    "    * model dyskryminatywny\n",
    "        $$\\ln\\left(\\prod_{n=1}^N p(y=y_n\\mid\\mathbf{x}=\\mathbf{x}_n,\\theta=\\widehat\\theta)\\right) = \\sum_{n=1}^N \\ln[p(y=y_n\\mid\\mathbf{x}=\\mathbf{x}_n,\\theta=\\widehat\\theta)]$$\n",
    "* logarytm jest funkcją __silnie rosnącą__ - maksymalizowanie likelihood jest __równoważne__ maksymalizowaniu log likelihood\n",
    "* zazwyczaj chcemy też zamienić sumę na __średnią__\n",
    "    * model generatywny\n",
    "        $$\\dfrac{1}{N}\\sum_{n=1}^N\\ln[p(\\mathbf{x}=\\mathbf{x}_n \\mid\\theta=\\widehat\\theta)]$$\n",
    "    * model dyskryminatywny\n",
    "        $$\\dfrac{1}{N}\\sum_{n=1}^N\\ln[p(y=y_n\\mid\\mathbf{x}=\\mathbf{x}_n,\\theta=\\widehat\\theta)]$$\n",
    "* średnia sprawia, że maksymalizowana funkcja ma __podobny rząd wielkości__ dla __różnych rozmiarów zbioru treningowego__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja kosztu\n",
    "\n",
    "1. chcemy __minimalizować__ funkcję kosztu\n",
    "2. chcemy __maksymalizować__ mean log likelihood\n",
    "3. najprostsze rozwiązanie - funkcja kosztu to __negative mean log likelihood__ (dodajemy znak minus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niech \n",
    "$$\\begin{align}\n",
    "p(x)&\\sim\\mathcal{N}(\\mu, \\sigma^2)\\\\\n",
    "p(x;\\mu, \\sigma^2)&\\sim\\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}\\left[-\\dfrac{1}{2}\\left(\\dfrac{x-\\mu}{\\sigma}\\right)^2\\right]\n",
    "\\end{align}$$\n",
    "gdzie każde $p(x;\\mu,\\sigma^2)$ jest likelihood $x$ dla parametrów $[\\mu, \\sigma^2]$\n",
    "* chcemy znaleźć $\\theta$ takie, że z największą szansą będą generować te dane\n",
    "* w ten sposób problemem jest maksymalizacja (maximum likelihood estimation, MLE) $$\\hat\\theta=\\arg\\max_\\theta\\mathcal{L}(\\theta\\mid x)=\\arg\\max_\\theta p_\\theta(x)$$\n",
    "* wychodząc z założenia IID możemy zapisać cel jako\n",
    "$$\n",
    "\\begin{align}\\hat\\theta&=\\arg\\max_\\theta\\prod_{i=1}^Np\\theta(x_i)=\\arg\\max_\\theta\\sum_{i=1}^N\\log{}p_\\theta(x_i)\\\\\n",
    "&=\\arg\\max_\\theta\\dfrac{1}{N}\\sum_{i=1}^N\\log{}p_\\theta(x_i)\n",
    "\\end{align}$$\n",
    "* z czego dostajemy **negative log-likelihood**\n",
    "$$\\hat\\theta=\\arg\\min_\\theta-\\mathbb{E}_{x\\sim{}p_{\\theta^\\ast}(x)}[\\log{}p_\\theta(x)]$$\n",
    "\n",
    "### model dyskryminatywny\n",
    "Dla rozkładu normalnego\n",
    "$$\\log{}p_\\theta(y\\mid x)=-N\\log\\sigma-\\dfrac{N}{2}\\log2\\pi-\\sum_{i=1}^N\\dfrac{\\|x_i^T\\theta-y_i\\|}{2\\sigma^2}$$\n",
    "  * tu występuje założenie o szumie o rozkładzie normalnym\n",
    "  * okazuje się, że średniokwadratowa funkcja kosztu jest bezpośrednią konsekwencją tego założenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uwagi\n",
    "* MLE daje jedynie estymacje punktowe funkcji kosztu i nie pozwala wnioskować na temat rozkładów kosztu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
