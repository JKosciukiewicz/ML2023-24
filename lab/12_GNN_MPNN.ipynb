{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Graph Neural Networks (GNNs)\n",
    "## Part I: Message Passing Neural Networks (MPNNs)\n",
    "We are going to implement few MPNNs for molecular property prediciton. It's recommended that you're familiar with the recent lectures on GNNs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bec6577a3d2a155"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Packages for GNNs\n",
    "There two very popular packages for GNNs that uses pytorch as a backend:\n",
    "1. [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html).\n",
    "2. [Deep Graph Library](https://www.dgl.ai/pages/start.html) (along with [dgl-lifesci](https://lifesci.dgl.ai/install/index.html).\n",
    "The former is more stable, the latter has a convenient extension [dgl-lifesci](https://lifesci.dgl.ai/generated/dgllife.utils.CanonicalAtomFeaturizer.html) for molecular data and is generally much more user-friendly. For convenience, we are going to use all three packages, so install appropriate versions of them, please (I recommend installing with pip). If you have issues with installing rdkit (required by dgl-lifesci), you can install rdkit using pip (pip install rdkit).\n",
    "\n",
    "Some additional packages that we are going to use:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a10f517285d85363"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# !pip install torchmetrics\n",
    "# !pip install wandb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:48.749731Z",
     "start_time": "2023-12-11T15:15:48.598779Z"
    }
   },
   "id": "dec783bf177280f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Molecular graphs\n",
    "(Copied from [mldd23 repository](https://github.com/gmum/mldd23/blob/main/labs/L3-graph-neural-networks/laboratory.ipynb))\n",
    "In mathematics, a graph is an object that consists of a set of vertices (nodes) connected with edges, i.e. $\\mathcal{G} = (V, E)$, where $V = \\{ v_i: i \\in \\{1, 2, \\dots, N \\} \\}$ and $E \\subseteq \\{ (v_i, v_j):\\, v_i,v_j \\in V \\}$.\n",
    "\n",
    "Molecular graphs are a special class of graphs, where besides nodes (denoting atoms) and edges (denoting chemical bonds), we have an additional information about atom types and sometimes also bond types. We can assume that we have an additional set of node/atom features encoded as a matrix $X$, where $X_{ij}$ is the $j$-th feature of the $i$-th atom. As atomic features, we can have one-hot encoded atom symbols (a vector containing zeros on all positions besides the position that corresponds to the atom symbol), the number of implicit hydrogens bonded with this atom, or the number of heavy neighbors (atoms other than hydrogens bonded to the given atom).\n",
    "\n",
    "Egdes/bonds can be encoded in two different ways. One method is to use an adjacency matrix $A$, where $A_{ij}=1$ if nodes/atoms $v_i$ nad $v_j$ are connected ($A_{ij}=0$ otherwise). In the case of sparse matrices, a more useful encoding is a list of pairs of connected atoms (a list of index pairs). This latter enocding is used by the PyTorch-Geometric library.\n",
    "\n",
    "In practice, a molecular graph can be described by two matrices: $X \\in \\mathbb{R}^{N \\times F}$ and $E \\in \\{0, 1,\\dots,N-1\\}^{2 \\times N}$, where $N$ is the number of atoms, and $F$ is the number of atomic features.\n",
    "<img src=\"resources/mol_graph.png\" height=\"500\" />"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61eff200e2fa43b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7533978cf98913d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are going to use FreeSolv dataset that contains 642 hydration free energy values for small molecules. The goal is to predict the [hydration free energy](https://en.wikipedia.org/wiki/Hydration_energy) of a given molecule. It's a very commonly used dataset for benchmarking molecular property prediction models. It's small, so we can minimize our co2 footprint and time spent on training. \n",
    "\n",
    "Molecules in most chemical datasets are represented with SMILES. SMILES is a linearization of the molecular graph, it's pretty convenient and can even be used as an input to text-based models. Fortunately, dgllife provides a fancy FreeSolv dataset wrapper that will 1) transform the SMILES into a molecular graph, and 2) encode the nodes and edges with some sensible chemical features (like atom types, bond type etc.) with node and edge features, so we don't really need to care about it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7eeba76a4a89736"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n"
     ]
    }
   ],
   "source": [
    "from dgllife.utils import CanonicalAtomFeaturizer, CanonicalBondFeaturizer, SMILESToBigraph\n",
    "from dgllife.data import FreeSolv\n",
    "import torch\n",
    "import dgl\n",
    "\n",
    "node_featurizer = CanonicalAtomFeaturizer()\n",
    "edge_featurizer = CanonicalBondFeaturizer(self_loop=True)\n",
    "dataset = FreeSolv(\n",
    "    smiles_to_graph=SMILESToBigraph(\n",
    "        node_featurizer=node_featurizer,\n",
    "        edge_featurizer=edge_featurizer,\n",
    "        add_self_loop=True,\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.562914Z",
     "start_time": "2023-12-11T15:15:48.616258Z"
    }
   },
   "id": "e6b97de5d9371cf2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Playground"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bf700b3346f6a31"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "('CN(C)C(=O)c1ccc(cc1)OC',\n Graph(num_nodes=13, num_edges=39,\n       ndata_schemes={'h': Scheme(shape=(74,), dtype=torch.float32)}\n       edata_schemes={'e': Scheme(shape=(13,), dtype=torch.float32)}),\n tensor([-11.0100]))"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles, graph, label = dataset[0]\n",
    "smiles, graph, label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.568451Z",
     "start_time": "2023-12-11T15:15:49.564290Z"
    }
   },
   "id": "fd08f424238c9580"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the dataset item consist of a SMILES string, a graph, and a label. The graph is a [DGLGraph](https://docs.dgl.ai/en/0.8.x/api/python/dgl.DGLGraph.html) object that contains node and edge features. We can access them with the following code:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b297de83ea20e626"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([13, 74])"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.ndata['h'].shape  # node features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.571379Z",
     "start_time": "2023-12-11T15:15:49.567706Z"
    }
   },
   "id": "f6cd6aafddd20202"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([39, 13])"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edata['e'].shape  # edge features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.585687Z",
     "start_time": "2023-12-11T15:15:49.571477Z"
    }
   },
   "id": "972195282cdfb48f"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[12,  0],\n        [ 0, 12],\n        [ 0,  2],\n        [ 2,  0],\n        [ 0,  4],\n        [ 4,  0],\n        [ 4,  7],\n        [ 7,  4],\n        [ 4,  9],\n        [ 9,  4],\n        [ 9,  6],\n        [ 6,  9],\n        [ 6, 10],\n        [10,  6],\n        [10, 11],\n        [11, 10],\n        [11,  3],\n        [ 3, 11],\n        [ 3,  8],\n        [ 8,  3],\n        [11,  5],\n        [ 5, 11],\n        [ 5,  1],\n        [ 1,  5],\n        [ 8,  9],\n        [ 9,  8],\n        [ 0,  0],\n        [ 1,  1],\n        [ 2,  2],\n        [ 3,  3],\n        [ 4,  4],\n        [ 5,  5],\n        [ 6,  6],\n        [ 7,  7],\n        [ 8,  8],\n        [ 9,  9],\n        [10, 10],\n        [11, 11],\n        [12, 12]], dtype=torch.int32)"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_nodes, end_nodes = graph.edges()  # edges. Note that edges are directed, so we have two edges for each bond. Moreover, we have self-loops, to easily handle molecules with only one atom.\n",
    "edges = torch.stack([start_nodes, end_nodes], dim=1)\n",
    "edges"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.585880Z",
     "start_time": "2023-12-11T15:15:49.574913Z"
    }
   },
   "id": "bf1af8de0703b2d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importantly, if we want to create a batch of graphs, we can simply treat the graphs as... a single graph with many disconnected components. The reason is that MPNN cannot pass the message between disconnected compontents, so the graphs in a batch won't influence each other. To make a batch from two graphs, we can simply run:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "611ba9d29599927d"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "(Graph(num_nodes=13, num_edges=39,\n       ndata_schemes={'h': Scheme(shape=(74,), dtype=torch.float32)}\n       edata_schemes={'e': Scheme(shape=(13,), dtype=torch.float32)}),\n Graph(num_nodes=5, num_edges=13,\n       ndata_schemes={'h': Scheme(shape=(74,), dtype=torch.float32)}\n       edata_schemes={'e': Scheme(shape=(13,), dtype=torch.float32)}),\n Graph(num_nodes=18, num_edges=52,\n       ndata_schemes={'h': Scheme(shape=(74,), dtype=torch.float32)}\n       edata_schemes={'e': Scheme(shape=(13,), dtype=torch.float32)}))"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, graph_1, _ = dataset[0]\n",
    "_, graph_2, _ = dataset[1]\n",
    "collated_graph = dgl.batch([graph_1, graph_2])\n",
    "graph_1, graph_2, collated_graph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.585967Z",
     "start_time": "2023-12-11T15:15:49.578783Z"
    }
   },
   "id": "1063ec4d70a6c648"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([13,  5])"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collated_graph.batch_num_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.659167Z",
     "start_time": "2023-12-11T15:15:49.583356Z"
    }
   },
   "id": "cd9357b97e4c3b1b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the collated_graph, the ids corresponding to the nodes of graph_2 are shifted by the size of graph_1:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b37a35a6ae91e55d"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=torch.int32),\n tensor([0, 1, 2, 3, 4], dtype=torch.int32),\n tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n        dtype=torch.int32))"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_1.nodes(), graph_2.nodes(), collated_graph.nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.673590Z",
     "start_time": "2023-12-11T15:15:49.586749Z"
    }
   },
   "id": "7598293d45c78c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split\n",
    "We are going to make our split slightly harder by using [scaffold](https://hub.knime.com/infocom/extensions/jp.co.infocom.cheminfo.jchem.feature/latest/jp.co.infocom.cheminfo.jchem.bemismurckoclustering.BemisMurckoClusteringNodeFactory) (scaffold is the largest cycle in a molecule) splitting that puts molecules with similar scaffolds to the same split."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e67b6515649fa77d"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start initializing RDKit molecule instances...\n",
      "Start computing Bemis-Murcko scaffolds.\n"
     ]
    }
   ],
   "source": [
    "from dgllife.utils import ScaffoldSplitter\n",
    "\n",
    "splitter = ScaffoldSplitter()\n",
    "train, valid, test = splitter.train_val_test_split(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.674260Z",
     "start_time": "2023-12-11T15:15:49.620132Z"
    }
   },
   "id": "901db3d9cdc5a748"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6a601dc47074ca6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trainer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be6593567ea5467"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torchmetrics import Metric\n",
    "from dgl.data import Subset\n",
    "from torch import nn\n",
    "from typing import Type\n",
    "from typing import Dict, Any\n",
    "from pathlib import Path\n",
    "from abc import ABC, abstractmethod\n",
    "from lab.checker import expected_mean_readout, expected_gin_layer_output, expected_sage_layer_output, \\\n",
    "    expected_attention_readout, expected_gine_layer_output, expected_sum_readout, expected_simple_mpnn_output\n",
    "\n",
    "\n",
    "class LoggerBase(ABC):\n",
    "    def __init__(self, logdir: str | Path):\n",
    "        self.logdir = Path(logdir)\n",
    "        self.logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    @abstractmethod\n",
    "    def log_metrics(self, metrics: Dict[str, Any], prefix: str):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "class DummyLogger(LoggerBase):  # If you don't want to use any logger, you can use this one\n",
    "    def log_metrics(self, metrics: Dict[str, Any], prefix: str):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def restart(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MetricList:\n",
    "    def __init__(self, metrics: Dict[str, Metric]):\n",
    "        self.metrics = copy.deepcopy(metrics)\n",
    "\n",
    "    def update(self, preds: torch.Tensor, targets: torch.Tensor) -> None:\n",
    "        for name, metric in self.metrics.items():\n",
    "            metric.update(preds.detach().cpu(), targets.cpu())\n",
    "\n",
    "    def compute(self) -> Dict[str, float]:\n",
    "        metrics = {}\n",
    "        for name, metric_fn in self.metrics.items():\n",
    "            metrics[name] = metric_fn.compute().item()\n",
    "            metric_fn.reset()\n",
    "        return metrics\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            run_dir: str | Path,\n",
    "            train_dataset: Subset,\n",
    "            valid_dataset: Subset,\n",
    "            train_metrics: Dict[str, Metric],\n",
    "            valid_metrics: Dict[str, Metric],\n",
    "            model: nn.Module,\n",
    "            logger: LoggerBase,\n",
    "            optimizer_kwargs: Dict[str, Any],\n",
    "            optimizer_cls: Type[torch.optim.Optimizer] = torch.optim.Adam,\n",
    "            n_epochs: int,\n",
    "            train_batch_size: int = 32,\n",
    "            valid_batch_size: int = 16,\n",
    "            device: str = \"cuda\",\n",
    "            valid_every_n_epochs: int = 1,\n",
    "            loss_fn=nn.MSELoss()\n",
    "    ):\n",
    "        self.run_dir = Path(run_dir)\n",
    "        self.train_loader = GraphDataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        self.valid_loader = GraphDataLoader(\n",
    "            dataset=valid_dataset,\n",
    "            batch_size=valid_batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        self.train_metrics = MetricList(train_metrics)\n",
    "        self.valid_metrics = MetricList(valid_metrics)\n",
    "        self.logger = logger\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer_cls(model.parameters(), **optimizer_kwargs)\n",
    "        self.n_epochs = n_epochs\n",
    "        self.device = device\n",
    "        self.valid_every_n_epochs = valid_every_n_epochs\n",
    "        self.loss_fn = loss_fn\n",
    "        self.model.to(device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, dataloader: GraphDataLoader, prefix: str) -> Dict[str, float]:\n",
    "        previous_mode = self.model.training\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        for _, graphs, labels in dataloader:\n",
    "            graphs = graphs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            preds = self.model(graphs)\n",
    "            loss = self.loss_fn(preds, labels)\n",
    "            losses.append(loss.item())\n",
    "            self.valid_metrics.update(preds, labels)\n",
    "        self.model.train(mode=previous_mode)\n",
    "        metrics = {\"loss\": np.mean(losses)} | self.valid_metrics.compute()\n",
    "        self.logger.log_metrics(metrics=metrics, prefix=prefix)\n",
    "        return metrics\n",
    "\n",
    "    def train(self) -> Dict[str, float]:\n",
    "        self.model.train()\n",
    "        valid_metrics = {}\n",
    "        for epoch in tqdm(range(self.n_epochs), total=self.n_epochs):\n",
    "            for _, graphs, labels in self.train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                graphs = graphs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                preds = self.model(graphs)\n",
    "                loss = self.loss_fn(preds, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                self.train_metrics.update(preds, labels)\n",
    "                train_metrics = {\"loss\": loss.item()} | self.train_metrics.compute()\n",
    "                self.logger.log_metrics(metrics=train_metrics, prefix=\"train\")\n",
    "\n",
    "                if epoch % self.valid_every_n_epochs == 0 or epoch == self.n_epochs - 1:\n",
    "                    valid_metrics = self.validate(self.valid_loader, prefix=\"valid\")\n",
    "\n",
    "        return valid_metrics\n",
    "\n",
    "    def test(self, dataset: Subset) -> Dict[str, float]:\n",
    "        dataloader = GraphDataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=16,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        return self.validate(dataloader, prefix=\"test\")\n",
    "\n",
    "    def close(self):  # close the logger, not really required for wandb\n",
    "        self.logger.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.674313Z",
     "start_time": "2023-12-11T15:15:49.633153Z"
    }
   },
   "id": "e75cf00937a3c413"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graph Neural Networks (GNNs)\n",
    "The high-level Graph Neural Network architecture we are going to use looks roughly like this:\n",
    "\n",
    "<img src=\"resources/gnn.png\" width=\"1200\" />\n",
    "\n",
    "- The Featurizer takes a molecule and transforms it to a graph with node and edge features (it happens at the level of dataset, so we don't really need to worry about that).\n",
    "- In our case, we will linearly embed the node and edge features to the hidden size before applying first MPNN layer which is not captured in the diagram.\n",
    "- The MPNN layer takes node (and possibly edge embeddings) and the graph structure and returns updated node embeddings. It happens in a loop.\n",
    "- Then the node embeddings are aggregated by the Readout layer to obtain a graph embeddings.\n",
    "- Finally, the graph embeddings are passed to the MLP to obtain the final prediction."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d54c9a1e68cd221"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "class MPNNLayerBase(ABC, nn.Module):\n",
    "    def _init(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, node_embeddings: torch.Tensor, edge_embeddings: torch.Tensor,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "\n",
    "class ReadoutBase(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, node_embeddings: torch.Tensor, graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_features_size: int,\n",
    "                 edge_features_size: int,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 mpnn_layer_cls: Type[MPNNLayerBase],\n",
    "                 mpnn_layer_kwargs: Dict[str, Any],\n",
    "                 mpnn_n_layers: int,\n",
    "                 readout_cls: Type[ReadoutBase]):\n",
    "        super().__init__()\n",
    "        self.linear_node = nn.Linear(node_features_size, hidden_size)\n",
    "        self.linear_edge = nn.Linear(edge_features_size, hidden_size)\n",
    "        self.mpnn_layers = nn.ModuleList([\n",
    "            mpnn_layer_cls(hidden_size=hidden_size, **mpnn_layer_kwargs)\n",
    "            for _ in range(mpnn_n_layers)\n",
    "        ])\n",
    "        self.readout = readout_cls(hidden_size=hidden_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        node_embeddings, edge_embeddings = graph.ndata['h'], graph.edata['e']\n",
    "        node_embeddings = self.linear_node(node_embeddings)\n",
    "        edge_embeddings = self.linear_edge(\n",
    "            edge_embeddings)  # some of the models does not use edge features, but we won't use if-clauses for convenience.\n",
    "        for layer in self.mpnn_layers:\n",
    "            node_embeddings = layer(node_embeddings=node_embeddings, edge_embeddings=edge_embeddings, graph=graph)\n",
    "        graph_embedding = self.readout(node_embeddings, graph)\n",
    "        predictions = self.mlp(graph_embedding)\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.674390Z",
     "start_time": "2023-12-11T15:15:49.638119Z"
    }
   },
   "id": "e6e7df76cc963d8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Readout\n",
    "Readout operation is used to aggregate node embeddings to obtain a graph embedding. There are many different readout operations, but the most popular are: sum, mean, attention, and max. We are going to implement the first three of them. Summing over nodes' embeddings seems trivial, but they're stored in a sparse format, meaning that all the nodes form all the graphs in a batch are stored in a one tensor of size `[num_nodes_1 + num_nodes_2 + ... + num_nodes_N, hidden_size]':   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9ac59d947b4000b"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([23, 16]), tensor([13,  5,  5]))"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_graph = dgl.batch([dataset[0][1], dataset[1][1], dataset[2][1]])\n",
    "linear = nn.Linear(node_featurizer.feat_size(), 16)\n",
    "node_embeddings = linear(batched_graph.ndata['h'])\n",
    "node_embeddings.shape, batched_graph.batch_num_nodes()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.675636Z",
     "start_time": "2023-12-11T15:15:49.641552Z"
    }
   },
   "id": "473300e58a6023b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For simplicity, we will convert the sparse node embeddings to a dense format with padding. Then the shape of the node embeddings will be `[batch_size, max_num_nodes, hidden_size]`. We can use the `to_dense_batch` function from `torch_geometric` for that:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fce302b7c444ae16"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "\n",
    "def to_dense_embeddings(node_embeddings: torch.Tensor, graph: dgl.DGLGraph, fill_value: float = 0.0) -> Tuple[\n",
    "    torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Converts sparse node embeddings to dense node embeddings with padding.\n",
    "    Arguments:\n",
    "        node_embeddings: node embeddings in a sparse format, i.e. [total_num_nodes, hidden_size]\n",
    "        graph: a batch of graphs\n",
    "        fill_value: a value to fill the padding with\n",
    "    Returns:\n",
    "        node_embeddings: node embeddings in a dense format, i.e. [batch_size, max_num_nodes, hidden_size]\n",
    "        mask: a mask indicating which nodes are real and which are padding, i.e. [batch_size, max_num_nodes]\n",
    "    \"\"\"\n",
    "    num_nodes = graph.batch_num_nodes() # e.g. [2, 3, 3]\n",
    "    indices = torch.arange(len(num_nodes), device=num_nodes.device)\n",
    "    batch = torch.repeat_interleave(indices, num_nodes).long() # e.g. [0, 0, 1, 1, 1, 2, 2, 2]\n",
    "    return to_dense_batch(node_embeddings, batch,\n",
    "                          fill_value=fill_value)  # that's the only reason we have torch_geometric in the requirements\n",
    "\n",
    "\n",
    "def to_sparse_embeddings(node_embeddings: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts dense node embeddings to sparse node embeddings.\n",
    "    Arguments:\n",
    "        node_embeddings: node embeddings in a dense format, i.e. [batch_size, max_num_nodes, hidden_size]\n",
    "        mask: a mask indicating which nodes are real and which are padding, i.e. [batch_size, max_num_nodes]\n",
    "    Returns:\n",
    "        node_embeddings: node embeddings in a sparse format, i.e. [total_num_nodes, hidden_size]\n",
    "    \"\"\"\n",
    "    return node_embeddings[mask]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.675719Z",
     "start_time": "2023-12-11T15:15:49.647698Z"
    }
   },
   "id": "a3fd81a5384d2ad0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can simply convert the node embeddings to a dense format and sum them $x = \\sum_i^n x_i$:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4190f0ff3fa0ae1f"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "class SumReadout(ReadoutBase):\n",
    "    def forward(self, node_embeddings: torch.Tensor, graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        # We can also use dgl.sum_nodes function, but let assume it's forbidden in that notebook ;)\n",
    "        node_embeddings, _ = to_dense_embeddings(node_embeddings, graph)\n",
    "        return node_embeddings.sum(dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.675774Z",
     "start_time": "2023-12-11T15:15:49.651743Z"
    }
   },
   "id": "841a8932a12397f3"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def test_readout(readout_cls: Type[ReadoutBase], expected_output: torch.Tensor):\n",
    "    torch.manual_seed(0)\n",
    "    graph = dgl.batch([dataset[0][1], dataset[1][1], dataset[2][1]])\n",
    "    linear = nn.Linear(node_featurizer.feat_size(), 16)\n",
    "    node_embeddings = linear(graph.ndata['h'])\n",
    "    readout = readout_cls(hidden_size=16)\n",
    "    result = readout(node_embeddings, graph)\n",
    "    assert torch.allclose(result, expected_output, atol=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.675818Z",
     "start_time": "2023-12-11T15:15:49.656700Z"
    }
   },
   "id": "447a28b93e6e8e89"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "test_readout(SumReadout, expected_sum_readout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.737736Z",
     "start_time": "2023-12-11T15:15:49.659561Z"
    }
   },
   "id": "b5dd5e1b3bbde2cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1. Implement mean readout (1 point).\n",
    "Implement the mean readout given by formula $x = \\frac{1}{n}\\sum_i^n x_i$:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "579fd18386daaed"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "class MeanReadout(ReadoutBase):\n",
    "    def forward(self, node_embeddings: torch.Tensor, graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        # Don't use any dlg functions here\n",
    "        pass\n",
    "    \n",
    "test_readout(MeanReadout, expected_mean_readout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.738451Z",
     "start_time": "2023-12-11T15:15:49.667644Z"
    }
   },
   "id": "13bd85d993f87d63"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2. Implement attention readout (2 points).\n",
    "Implement the attention readout given by formula $x = \\sum_i^n \\frac{\\exp(score_i))}{\\sum_j^n \\exp(score_j)}x_i$, where $score_i=score\\_mlp(x_i)$:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db68abdbc2736d7a"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "class AttentionReadout(ReadoutBase):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__(hidden_size)\n",
    "        self.score_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, node_embeddings: torch.Tensor, graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        pass\n",
    "    \n",
    "test_readout(AttentionReadout, expected_attention_readout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.779192Z",
     "start_time": "2023-12-11T15:15:49.674122Z"
    }
   },
   "id": "6c1f5bcdbc90881"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Message Passing Neural Networks (MPNNs)\n",
    "Message Passing is given by formula: \n",
    "$$\n",
    "x'_i=\\rho(x_i, \\square_{j\\in N(i)} \\psi(x_j, x_i, e_{ji})),\n",
    "$$ \n",
    "where $\\psi$ is learnable message function, $\\rho$ is learnable update, and $\\square$ is aggregation function. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac465a3b9279474b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple MPNN\n",
    "For instance, we can define a very simple MPNN layer by the following formula:\n",
    "$$\n",
    "x'_i=W_1x_i + W_2\\sum_{j\\in N(i)} W_3x_j,\n",
    "$$\n",
    "where W_i are linear layers with implicit bias term (we will make the bias implicit in every formula in that notebook). Let us implement this simple MPNN:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "307c236e3cb84e76"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "class SimpleMPNNLayer(MPNNLayerBase):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_3 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self,\n",
    "                node_embeddings: torch.Tensor,\n",
    "                edge_embeddings: torch.Tensor,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        # graph is bi-directed, so we can freely swap the \"start\" and \"end\" meanings\n",
    "        start_nodes, end_nodes = graph.edges(order='srcdst') # using this `order` value sorts the `start_nodes`\n",
    "        messages = self.linear_3(node_embeddings[end_nodes]) # W_3x_j\n",
    "        message_dense, _ = to_dense_batch(messages, start_nodes.long(), fill_value=0.0) # to make the life easier, we convert the node embeddings to dense representation\n",
    "        aggregated_message = message_dense.sum(dim=1) # \\sum_{j\\in N(i)} W_3x_j\n",
    "        aggregated_message = self.linear_2(aggregated_message) # W_2\\sum_{j\\in N(i)} W_3x_j\n",
    "        node_embeddings = self.linear_1(node_embeddings) + aggregated_message # W_1x_i + W_2\\sum_{j\\in N(i)} W_3x_j\n",
    "        return node_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.779483Z",
     "start_time": "2023-12-11T15:15:49.679591Z"
    }
   },
   "id": "f11f284c299831ed"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "def test_mpnn_layer(mpnn_layer_cls: Type[MPNNLayerBase], expected_output: torch.Tensor):\n",
    "    torch.manual_seed(0)\n",
    "    graph = dgl.batch([dataset[0][1], dataset[1][1]])\n",
    "    linear_nodes = nn.Linear(node_featurizer.feat_size(), 4)\n",
    "    linear_edges = nn.Linear(edge_featurizer.feat_size(), 4)\n",
    "    node_embeddings = linear_nodes(graph.ndata['h'])\n",
    "    edge_embeddings = linear_edges(graph.edata['e'])\n",
    "    layer = mpnn_layer_cls(hidden_size=4)\n",
    "    result = layer(node_embeddings, edge_embeddings, graph)\n",
    "    assert torch.allclose(result, expected_output, atol=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.779529Z",
     "start_time": "2023-12-11T15:15:49.683224Z"
    }
   },
   "id": "8767bffa62008f67"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "test_mpnn_layer(SimpleMPNNLayer, expected_simple_mpnn_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.779871Z",
     "start_time": "2023-12-11T15:15:49.685486Z"
    }
   },
   "id": "461980b684728501"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 3. Implement GraphSAGE layer (2 points).\n",
    "Implement a GraphSAGE given by the following formula:\n",
    "$$\n",
    "x'_i=W_1x_i + W_2\\frac{1}{deg(i)}\\sum_{j\\in N(i)} x_j,\n",
    "$$\n",
    "where $deg(i) = #N(i)$ is the number of neighbors of node $i$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0dbbc2f2efa8ca2"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "class SAGELayer(MPNNLayerBase):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self,\n",
    "                node_embeddings: torch.Tensor,\n",
    "                edge_embeddings: torch.Tensor,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "test_mpnn_layer(SAGELayer, expected_sage_layer_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.779946Z",
     "start_time": "2023-12-11T15:15:49.691201Z"
    }
   },
   "id": "82528a8dacf24cd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 4. Implement GIN layer (2 points).\n",
    "Implement a GIN layer given by the following formula:\n",
    "$$\n",
    "x'_i=mlp((1 + \\epsilon)x_i + \\sum_{j\\in N(i)} x_j).\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea8735b272ecad46"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "class GINLayer(MPNNLayerBase):\n",
    "    def __init__(self, hidden_size: int, eps: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.eps = eps\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                node_embeddings: torch.Tensor,\n",
    "                edge_embeddings: torch.Tensor,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "test_mpnn_layer(GINLayer, expected_gin_layer_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.779991Z",
     "start_time": "2023-12-11T15:15:49.697036Z"
    }
   },
   "id": "e422bea1856fc29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 5. Implement GINE layer (2 points).\n",
    "Implement a GINE layer given by the following formula:\n",
    "$$\n",
    "x'_i=mlp((1 + \\epsilon)x_i + \\sum_{j\\in N(i)} ReLU(x_j + e_{ji})).\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fe4ec2134b539da"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "class GINELayer(MPNNLayerBase):\n",
    "    def __init__(self, hidden_size: int, eps: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.eps = eps\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                node_embeddings: torch.Tensor,\n",
    "                edge_embeddings: torch.Tensor,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        start_nodes, end_nodes, edge_ids = graph.edges(order='srcdst', form='all')\n",
    "        pass\n",
    "\n",
    "test_mpnn_layer(GINELayer, expected_gine_layer_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.780022Z",
     "start_time": "2023-12-11T15:15:49.702353Z"
    }
   },
   "id": "635e6bcb4e40fdd2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b220afd7f1d78b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logger\n",
    "We are going to use [wandb](https://wandb.ai/site) for logging. It's a very convenient tool for logging and visualizing the training process. It's free for academic use, so you can create an account and use it for your projects. If you don't want to use wandb, you can use any other online logger (like [comet.ml](https://www.comet.ml/site/)), but you need to implement the appropriate LoggerBase subclass on your own. To setup and use wandb, you need to do the following:\n",
    "1. [Setup the wandb](https://docs.wandb.ai/quickstart) (or any other online logger).\n",
    "2. Give your supervisor access to your project (ask him/her about the username.\n",
    "3. Use the logger for all your trainings and provide the links to the final runs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f465e5d378487e7"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "class WandbLogger(LoggerBase):\n",
    "    def __init__(\n",
    "            self, logdir: str | Path, project_name: str, experiment_name: str, **kwargs: Dict[str, Any]\n",
    "    ):\n",
    "        super().__init__(logdir)\n",
    "        import wandb\n",
    "        self.project_name = project_name\n",
    "        self.experiment_name = experiment_name\n",
    "        self.kwargs = kwargs\n",
    "        self.run = wandb.init(\n",
    "            dir=self.logdir,\n",
    "            project=self.project_name,\n",
    "            name=self.experiment_name,\n",
    "            **self.kwargs,\n",
    "        )\n",
    "\n",
    "    def log_metrics(self, metrics: Dict[str, Any], prefix: str):\n",
    "        metrics = {f\"{prefix}/{k}\": v for k, v in metrics.items()}\n",
    "        self.run.log(metrics)\n",
    "\n",
    "    def close(self):\n",
    "        self.run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.780089Z",
     "start_time": "2023-12-11T15:15:49.708043Z"
    }
   },
   "id": "1462ef331426d529"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 6. Train GraphSAGE (2 points).\n",
    "1. Tune hyperparameters of a GNN with `SAGELayer` as MPNN layer to obtain at most 2.0 MAE on the validation set. You can modify the GNN/MPNN architecture, so it uses some regularization tricks like dropout or batch norm. Don't change the validation batch size. If your validation MAE is in (2.0, 2.5], you can obtain 1 point.\n",
    "2. Report the obtained MAE on the validation and test set (only the former need to be lower than 2.0 MAE).\n",
    "3. Provide the link to the final run: [your link]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1db91eaab1a90175"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "### Example code for training. You can modify it for easier grid-searching."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.780133Z",
     "start_time": "2023-12-11T15:15:49.710445Z"
    }
   },
   "id": "e3e9e0cf69701d46"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_time_stamp() -> str:\n",
    "    return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T15:15:49.780168Z",
     "start_time": "2023-12-11T15:15:49.713308Z"
    }
   },
   "id": "2980b66ad10a9824"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchmetrics import MeanAbsoluteError as MAE\n",
    "from torchmetrics import MeanSquaredError as MSE\n",
    "from torchmetrics import PearsonCorrCoef as PCC\n",
    "\n",
    "metrics = {\n",
    "    \"mae\": MAE(),\n",
    "    \"mse\": MSE(),\n",
    "    \"pcc\": PCC(),\n",
    "}\n",
    "\n",
    "model = GNN(\n",
    "    node_features_size=node_featurizer.feat_size(),\n",
    "    edge_features_size=edge_featurizer.feat_size(),\n",
    "    hidden_size=256,\n",
    "    output_size=1,\n",
    "    mpnn_layer_cls=SAGELayer,\n",
    "    mpnn_n_layers=6,\n",
    "    readout_cls=MeanReadout,\n",
    "    mpnn_layer_kwargs={}\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    run_dir=\"experiments\",\n",
    "    train_dataset=train,\n",
    "    valid_dataset=valid,\n",
    "    train_metrics=metrics,\n",
    "    valid_metrics=metrics,\n",
    "    train_batch_size=32,\n",
    "    model=model,\n",
    "    logger=WandbLogger(\n",
    "        logdir=\"runs/mpnn\",\n",
    "        project_name=\"mldd23\",\n",
    "        experiment_name=f\"sage_{get_time_stamp()}\",\n",
    "    ),\n",
    "    optimizer_kwargs={\"lr\": 1e-4},\n",
    "    n_epochs=50,\n",
    "    device=\"cpu\",\n",
    "    valid_every_n_epochs=1,\n",
    ")\n",
    "\n",
    "valid_metrics = trainer.train()\n",
    "test_metrics = trainer.test(test)\n",
    "trainer.close()\n",
    "print(f\"Validation metrics: {valid_metrics}\")\n",
    "print(f\"Test metrics: {test_metrics}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18cd039ad8f03ca6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 7. Train GIN (2 points).\n",
    "1. Tune hyperparameters of a GNN with `GINLayer` as MPNN layer to obtain at most 2.0 MAE on the validation set. You can modify the GNN/MPNN architecture, so it uses some regularization tricks like dropout or batch norm. Don't change the validation batch size. If your validation MAE is in (2.0, 2.5], you can obtain 1 point.\n",
    "2. Report the obtained MAE on the validation and test set (only the former need to be lower than 2.0 MAE).\n",
    "3. Provide the link to the final run: [your link]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0acbbdb6f55d814"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 8. Train GINE (2 points).\n",
    "1. Tune hyperparameters of a GNN with `GINELayer` as MPNN layer to obtain at most 2.0 MAE on the validation set. You can modify the GNN/MPNN architecture, so it uses some regularization tricks like dropout or batch norm. Don't change the validation batch size. If your validation MAE is in (2.0, 2.5], you can obtain 1 point.\n",
    "2. Report the obtained MAE on the validation and test set (only the former need to be lower than 2.0 MAE).\n",
    "3. Provide the link to the final run: [your link]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "100ee2cd44877b2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code optimization\n",
    "Some pieces of code were written suboptimally. Your task is to slightly optimize them. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "802615e3afaddf44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 9. Optimize SumReadout (1 point).\n",
    "`SumReadout` was written using `to_dense_embeddings` function which does some unecessary memory allocations and computations. Your task is to rewrite the method using code from a bare torch library. Hint: `torch.index_add`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17c64785b23a2b1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class OptimizedSumReadout(ReadoutBase):\n",
    "    def forward(self, node_embeddings: torch.Tensor, graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "test_readout(OptimizedSumReadout, expected_sum_readout)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "292cb0197e977862"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 10. Optimize MeanReadout (1 point).\n",
    "Your task is to rewrite the method using code from a bare torch library."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4be6a2ed6011b64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class OptimizedMeanReadout(ReadoutBase):\n",
    "    def forward(self, node_embeddings: torch.Tensor, graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "\n",
    "test_readout(OptimizedMeanReadout, expected_mean_readout)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9baf5944d7ad42c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 11. Optimize SimpleMPNNLayer (1 point).\n",
    "We can make our implementations of `SimpleMPNNLayer` layer (and basically any other MPNN layer) slightly faster by:\n",
    "- reducing the costs of the message embedding (in the case of `SimpleMPNNLayer`, it's application of `self.linear_3`) from $O(m)$ to $O(n)$, where $m$ is the number of edges in the graph and $n$ is the number of nodes.\n",
    "- removing quite expensive `to_dense_batch` call.\n",
    "\n",
    "Your task is to apply the above optimizations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b0f8cab6725a285"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class OptimizedSimpleMPNNLayer(MPNNLayerBase):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_3 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self,\n",
    "                node_embeddings: torch.Tensor,\n",
    "                edge_embeddings: torch.Tensor,\n",
    "                graph: dgl.DGLGraph) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "test_mpnn_layer(OptimizedSimpleMPNNLayer, expected_simple_mpnn_output)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "862db51e45f8abf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
